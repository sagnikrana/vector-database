{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import chromadb\n",
    "from openai import OpenAI\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from chromadb.utils import embedding_functions\n",
    "load_dotenv()\n",
    "openai_client = OpenAI(api_key=os.getenv('OPEN_AI_API_KEY'))\n",
    "from chromadb.utils import embedding_functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_file_path = '../data/text-files'\n",
    "\n",
    "def split_text(text, chunk_size=1000, chunk_overlap=20):\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    while start < len(text):\n",
    "        end = start + chunk_size\n",
    "        chunks.append(text[start:end])\n",
    "        start = end - chunk_overlap\n",
    "    return chunks\n",
    "\n",
    "def load_documents_from_directory(directory_path):\n",
    "    print(\"==== Loading documents from directory ====\")\n",
    "    documents = []\n",
    "    for filename in os.listdir(directory_path):\n",
    "        if filename.endswith(\".txt\"):\n",
    "            with open(\n",
    "                os.path.join(directory_path, filename), \"r\", encoding=\"utf-8\"\n",
    "            ) as file:\n",
    "                documents.append({\"id\": filename, \"text\": file.read()})\n",
    "    return documents\n",
    "\n",
    "def  get_chroma_db_embedding(embedding_function_object, text):\n",
    "    embedding = embedding_function_object(text)\n",
    "    return embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the client\n",
    "chroma_client = chromadb.PersistentClient('../data/chroma_persist.db')\n",
    "collection_name = 'test_collection'\n",
    "default_ef = embedding_functions.DefaultEmbeddingFunction()\n",
    "collection = chroma_client.get_or_create_collection(collection_name, embedding_function=default_ef)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== Loading documents from directory ====\n",
      "Loaded: 21 documents\n",
      "==== Splitting docs into chunks ====\n",
      "==== Generating embeddings... ====\n",
      "Generating embeddings i2s done\n",
      "Generating embeddings i2s done\n",
      "Generating embeddings i2s done\n",
      "Generating embeddings i2s done\n",
      "Generating embeddings i2s done\n"
     ]
    }
   ],
   "source": [
    "# Loading the txt files from the document into the chroma db\n",
    "documents = load_documents_from_directory(text_file_path)\n",
    "print(f'Loaded: {len(documents)} documents')\n",
    "\n",
    "# Split the documents into chuks\n",
    "chunked_documents = []\n",
    "print(\"==== Splitting docs into chunks ====\")\n",
    "for doc in documents:\n",
    "    chunks = split_text(doc[\"text\"])\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        chunked_documents.append({\"id\": f\"{doc['id']}_chunk{i+1}\", \"text\": chunk})\n",
    "\n",
    "# Generate embeddings for the document chunks\n",
    "print(\"==== Generating embeddings... ====\")\n",
    "chunked_documents = chunked_documents[0:5]\n",
    "for index, doc in enumerate(chunked_documents):\n",
    "    doc[\"embedding\"] = get_chroma_db_embedding(default_ef, doc['text'])\n",
    "    print('Generating embeddings i2s done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== Inserting chunks into db ====\n",
      "==== Inserting chunks into db ====\n",
      "==== Inserting chunks into db ====\n",
      "==== Inserting chunks into db ====\n",
      "==== Inserting chunks into db ====\n"
     ]
    }
   ],
   "source": [
    "# Load the chunked documents into the database\n",
    "for doc in chunked_documents:\n",
    "    print(\"==== Inserting chunks into db ====\")\n",
    "    collection.upsert(ids=[doc[\"id\"]], documents=[doc[\"text\"]], embeddings=[doc[\"embedding\"][0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query Documents from the data inserted in the chroma database:\n",
    "\n",
    "def query_documents_chroma_db(question, n_result=2):\n",
    "    results = collection.query(query_texts=question, n_results=n_result)\n",
    "    relevant_chunks = [doc for sublist in results[\"documents\"] for doc in sublist]\n",
    "    return relevant_chunks\n",
    "\n",
    "question = 'What does Comedian Yedoye Travis sees for AI?'\n",
    "\n",
    "result = query_documents_chroma_db(question)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " dashboards for this purpose, while over half say that they’re investing in supply chain visibility services more broadly.\n",
      "\n",
      "Pando aims to meet the need by consolidating supply chain data that resides in multiple silos within and outside of the enterprise, including data on customers, suppliers, logistics service providers, facilities and product SKUs. The platform provides various tools and apps for accomplishing different tasks across freight procurement, trade and transport management, freight audit and payment and document management, as well as dispatch planning and analytics.\n",
      "\n",
      "Customers can customize the tools and apps or build their own using Pando’s APIs. This, along with the platform’s emphasis on no-code capabilities, differentiates Pando from incumbents like SAP, Oracle, Blue Yonder and E2Open, Jayakrishnan asserts.\n",
      "\n",
      "“Pando comes pre-integrated with leading enterprise resource planning (ERPs) systems and has ready APIs and a professional services team to integrate with any ne\n"
     ]
    }
   ],
   "source": [
    "print(result[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GenAI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
