{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import chromadb\n",
    "from openai import OpenAI\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from chromadb.utils import embedding_functions\n",
    "load_dotenv()\n",
    "openai_client = OpenAI(api_key=os.getenv('OPEN_AI_API_KEY'))\n",
    "from chromadb.utils import embedding_functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_file_path = '../data/text-files'\n",
    "\n",
    "def split_text(text, chunk_size=1000, chunk_overlap=20):\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    while start < len(text):\n",
    "        end = start + chunk_size\n",
    "        chunks.append(text[start:end])\n",
    "        start = end - chunk_overlap\n",
    "    return chunks\n",
    "\n",
    "def load_documents_from_directory(directory_path):\n",
    "    print(\"==== Loading documents from directory ====\")\n",
    "    documents = []\n",
    "    for filename in os.listdir(directory_path):\n",
    "        if filename.endswith(\".txt\"):\n",
    "            with open(\n",
    "                os.path.join(directory_path, filename), \"r\", encoding=\"utf-8\"\n",
    "            ) as file:\n",
    "                documents.append({\"id\": filename, \"text\": file.read()})\n",
    "    return documents\n",
    "\n",
    "def  get_chroma_db_embedding(embedding_function_object, text):\n",
    "    embedding = embedding_function_object(text)\n",
    "    return embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the client\n",
    "chroma_client = chromadb.PersistentClient('../data/chroma_persist.db')\n",
    "collection_name = 'test_collection'\n",
    "default_ef = embedding_functions.DefaultEmbeddingFunction()\n",
    "collection = chroma_client.get_or_create_collection(collection_name, embedding_function=default_ef)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== Loading documents from directory ====\n",
      "Loaded: 21 documents\n",
      "==== Splitting docs into chunks ====\n",
      "==== Generating embeddings... ====\n",
      "Generating embeddings i2s done\n",
      "Generating embeddings i2s done\n",
      "Generating embeddings i2s done\n",
      "Generating embeddings i2s done\n",
      "Generating embeddings i2s done\n"
     ]
    }
   ],
   "source": [
    "# Loading the txt files from the document into the chroma db\n",
    "documents = load_documents_from_directory(text_file_path)\n",
    "print(f'Loaded: {len(documents)} documents')\n",
    "\n",
    "# Split the documents into chuks\n",
    "chunked_documents = []\n",
    "print(\"==== Splitting docs into chunks ====\")\n",
    "for doc in documents:\n",
    "    chunks = split_text(doc[\"text\"])\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        chunked_documents.append({\"id\": f\"{doc['id']}_chunk{i+1}\", \"text\": chunk})\n",
    "\n",
    "# Generate embeddings for the document chunks\n",
    "print(\"==== Generating embeddings... ====\")\n",
    "chunked_documents = chunked_documents[0:5]\n",
    "for index, doc in enumerate(chunked_documents):\n",
    "    doc[\"embedding\"] = get_chroma_db_embedding(default_ef, doc['text'])\n",
    "    print('Generating embeddings i2s done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== Inserting chunks into db ====\n",
      "==== Inserting chunks into db ====\n",
      "==== Inserting chunks into db ====\n",
      "==== Inserting chunks into db ====\n",
      "==== Inserting chunks into db ====\n"
     ]
    }
   ],
   "source": [
    "# Load the chunked documents into the database\n",
    "for doc in chunked_documents:\n",
    "    print(\"==== Inserting chunks into db ====\")\n",
    "    collection.upsert(ids=[doc[\"id\"]], documents=[doc[\"text\"]], embeddings=[doc[\"embedding\"][0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GenAI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
