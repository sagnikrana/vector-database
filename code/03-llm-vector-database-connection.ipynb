{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import chromadb\n",
    "from openai import OpenAI\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from chromadb.utils import embedding_functions\n",
    "load_dotenv()\n",
    "openai_client = OpenAI(api_key=os.getenv('OPEN_AI_API_KEY'))\n",
    "from chromadb.utils import embedding_functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_file_path = '../data/text-files'\n",
    "\n",
    "def split_text(text, chunk_size=1000, chunk_overlap=20):\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    while start < len(text):\n",
    "        end = start + chunk_size\n",
    "        chunks.append(text[start:end])\n",
    "        start = end - chunk_overlap\n",
    "    return chunks\n",
    "\n",
    "def load_documents_from_directory(directory_path):\n",
    "    print(\"==== Loading documents from directory ====\")\n",
    "    documents = []\n",
    "    for filename in os.listdir(directory_path):\n",
    "        if filename.endswith(\".txt\"):\n",
    "            with open(\n",
    "                os.path.join(directory_path, filename), \"r\", encoding=\"utf-8\"\n",
    "            ) as file:\n",
    "                documents.append({\"id\": filename, \"text\": file.read()})\n",
    "    return documents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the client\n",
    "chroma_client = chromadb.PersistentClient('../data/chroma_persist.db')\n",
    "collection_name = 'test_collection'\n",
    "default_ef = embedding_functions.DefaultEmbeddingFunction()\n",
    "collection = chroma_client.get_or_create_collection(collection_name, embedding_function=default_ef)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== Loading documents from directory ====\n",
      "Loaded: 21 documents\n"
     ]
    }
   ],
   "source": [
    "# Loading the txt files from the document into the chroma db\n",
    "documents = load_documents_from_directory(text_file_path)\n",
    "print(f'Loaded: {len(documents)} documents')\n",
    "\n",
    "# Split the documents into chunks\n",
    "chunked_documents = []\n",
    "for doc in documents:\n",
    "    chunks = split_text(doc[\"text\"])\n",
    "    print(\"==== Splitting docs into chunks ====\")\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        chunked_documents.append({\"id\": f\"{doc['id']}_chunk{i+1}\", \"text\": chunk})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [\n",
    "{'id' : 'doc1', 'text' : 'Hello world'},\n",
    "{'id' : 'doc2', 'text' : 'How are you doing today'},\n",
    "{'id' : 'doc3', 'text' : 'Goodbye, See you later'},\n",
    "{'id' : 'doc4', 'text' : 'Welcome again!'},\n",
    "]\n",
    "\n",
    "# Adding the documents into the collection\n",
    "\n",
    "for doc in documents:\n",
    "    collection.upsert(ids = doc['id'], documents = doc['text'])\n",
    "\n",
    "query = \"Hello\"\n",
    "results = collection.query(query_texts = [query], n_results=3)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GenAI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
